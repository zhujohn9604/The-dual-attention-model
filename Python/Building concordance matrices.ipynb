{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building concordance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have 200 companies, and for each company, we have constructed its market vector and technology vectors, respectively. To simulate this case, we are going to use numpy to randomly generate market vectors and technology vectors for these companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** M, N are lists containing 200 items, and each item is a 300d vector. Company_ids is unique identifiers for each companies (for the sake of simplification, we generate a sequence of numbers up to 200 using **range** function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of M is : 200\n",
      "The length of T is : 200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "M = np.random.random((200, 300))\n",
    "T = np.random.random((200, 300))\n",
    "\n",
    "Company_ids = range(200)\n",
    "\n",
    "M = list(M)\n",
    "T = list(T)\n",
    "\n",
    "M_dict = dict(zip(Company_ids, M))\n",
    "T_dict = dict(zip(Company_ids, T))\n",
    "\n",
    "print('The length of M is :', len(M))\n",
    "print('The length of T is :', len(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The market vector for the 1st company:  [4.14119885e-01 8.14707724e-01 6.55967345e-01 3.77237501e-01\n",
      " 9.79273426e-01 9.63288582e-01 7.66990194e-01 6.74640872e-01\n",
      " 7.80924022e-01 9.73029324e-01 5.88517334e-01 3.91016730e-02\n",
      " 3.01780178e-01 5.84168299e-01 4.17947776e-01 3.19350547e-01\n",
      " 3.60474206e-01 1.85149974e-01 3.21713689e-01 1.07509597e-01\n",
      " 1.88005271e-01 6.14792377e-01 9.90431454e-01 7.16647647e-01\n",
      " 2.33173114e-01 9.03261748e-01 3.70825422e-01 2.20217737e-01\n",
      " 8.50122151e-01 4.57710383e-01 2.49403374e-01 2.13756121e-01\n",
      " 7.43894673e-01 2.85588032e-01 9.56516845e-01 2.21813243e-02\n",
      " 6.66949389e-03 8.33781832e-01 6.13170218e-01 8.94071388e-01\n",
      " 1.69366744e-01 7.31489159e-02 2.75569870e-01 7.78758713e-01\n",
      " 9.87762735e-01 8.90797455e-01 8.59343375e-01 5.19138177e-01\n",
      " 5.72995073e-01 7.52481540e-01 2.51458464e-01 7.25372261e-01\n",
      " 5.87796963e-01 1.01631766e-01 8.88869950e-02 3.19329991e-01\n",
      " 5.61488068e-02 7.25663629e-01 6.40863417e-01 3.37013836e-01\n",
      " 7.96715719e-02 2.50792251e-01 7.47884557e-01 4.88361745e-01\n",
      " 4.44469353e-01 6.28066511e-01 8.55566849e-01 8.59265018e-01\n",
      " 4.49837024e-02 2.08948880e-01 9.84396861e-01 1.30766082e-01\n",
      " 2.27760001e-01 1.75474338e-01 7.50092992e-01 8.20527508e-01\n",
      " 8.25761135e-01 3.53149141e-01 9.32627700e-01 8.36399418e-01\n",
      " 4.55464349e-01 9.81558408e-01 3.45626571e-01 5.01482570e-01\n",
      " 2.17819091e-01 5.15875511e-01 4.75536342e-01 2.63561533e-01\n",
      " 1.43998085e-01 5.91121799e-01 1.78638436e-01 5.67183217e-01\n",
      " 3.38428303e-01 3.19566158e-01 5.02359894e-01 3.19599586e-01\n",
      " 4.10120052e-01 5.58337237e-01 3.25911282e-01 8.63013600e-01\n",
      " 1.10195065e-01 7.73436807e-01 9.71937759e-01 8.14025165e-02\n",
      " 8.17688585e-01 6.75521486e-01 7.65491158e-04 2.27169336e-01\n",
      " 5.62058883e-01 5.28071362e-01 9.58559611e-01 4.03709219e-01\n",
      " 5.88040859e-01 6.77194081e-01 6.40679352e-01 5.31361488e-01\n",
      " 3.08361397e-01 4.96183598e-01 1.18709083e-01 7.80086006e-01\n",
      " 2.60011226e-01 7.27973313e-01 2.61232869e-02 9.07620681e-01\n",
      " 4.32432079e-01 1.08446737e-01 5.56977614e-01 8.51377477e-01\n",
      " 4.04167997e-01 8.07987036e-01 5.55126173e-01 2.06306987e-01\n",
      " 9.68439009e-01 3.72285432e-01 1.94856360e-01 5.55337677e-01\n",
      " 9.52346851e-01 7.85765217e-01 1.82053114e-01 1.88079249e-01\n",
      " 9.08708201e-01 7.99191326e-01 2.42451127e-01 8.04924627e-01\n",
      " 9.03457835e-01 8.40489727e-02 9.11796006e-01 7.96794699e-01\n",
      " 3.28762235e-01 6.89958476e-01 2.53196613e-01 2.39578613e-01\n",
      " 9.98129985e-01 3.64783875e-01 7.63954545e-02 6.53808808e-03\n",
      " 4.25087163e-01 1.18179271e-01 1.61919855e-01 3.17502744e-01\n",
      " 3.41526836e-01 8.59553660e-01 1.98821770e-01 1.27314858e-02\n",
      " 3.01436852e-01 7.49108908e-01 9.85092515e-01 2.23063430e-01\n",
      " 3.82070281e-01 2.44159990e-01 1.18564536e-02 4.19700920e-01\n",
      " 6.43200179e-01 9.51682119e-01 9.35996142e-01 3.12728441e-01\n",
      " 3.21369361e-01 3.34533313e-01 5.20972514e-01 6.48112800e-01\n",
      " 3.01521942e-01 6.17657734e-01 5.76664013e-01 5.12615521e-01\n",
      " 7.93585764e-01 7.97576119e-01 6.05130940e-01 9.51999433e-01\n",
      " 5.14597596e-01 9.25096328e-01 6.27826008e-01 7.64534368e-01\n",
      " 2.21867948e-01 6.33655455e-01 6.12256551e-01 1.57008907e-01\n",
      " 5.82275361e-01 8.71435675e-01 1.37974339e-01 5.04515702e-01\n",
      " 5.24408718e-01 4.43533160e-02 4.85598370e-01 9.94014694e-01\n",
      " 7.55333034e-01 5.52664593e-01 8.52022833e-01 9.54666047e-01\n",
      " 2.32631331e-01 1.59211524e-01 1.27738144e-02 6.24509869e-01\n",
      " 5.54341439e-01 9.14527664e-01 9.54143599e-01 4.33814185e-01\n",
      " 2.83670852e-01 2.93623364e-01 8.45290249e-01 1.50149692e-01\n",
      " 6.72128738e-01 3.64665297e-01 2.09547967e-01 5.92126884e-01\n",
      " 2.23347408e-01 7.86198163e-01 2.07877278e-01 1.94376172e-01\n",
      " 7.57676171e-01 7.13421779e-02 9.29094944e-01 9.48619154e-01\n",
      " 9.62598255e-01 9.85811863e-01 2.73231841e-01 1.60382253e-01\n",
      " 2.61999177e-01 8.23136207e-01 7.16045001e-01 6.83651503e-01\n",
      " 9.28803029e-01 4.38011219e-01 8.54390911e-01 6.10645670e-01\n",
      " 8.85458981e-01 2.87923760e-01 9.88657640e-02 6.12732022e-01\n",
      " 1.84208910e-01 1.32785857e-01 2.62339182e-01 1.89124879e-01\n",
      " 2.75039508e-01 7.35315010e-01 6.44061752e-01 5.35176714e-01\n",
      " 1.77793704e-02 5.01819613e-01 3.23805105e-01 6.90444814e-01\n",
      " 7.75192454e-01 5.66854881e-01 6.51968258e-01 7.10639429e-03\n",
      " 3.17608600e-02 8.93689740e-01 6.45404094e-01 5.38120074e-01\n",
      " 1.30472261e-02 7.14321805e-01 1.08764445e-01 8.67758911e-01\n",
      " 7.41641239e-01 2.83425846e-01 3.03668029e-02 7.31105939e-01\n",
      " 5.69191826e-01 2.21767740e-01 3.64461760e-01 1.45339320e-01\n",
      " 3.29930775e-01 7.99528620e-01 4.22642516e-01 1.28477610e-01\n",
      " 5.32210652e-01 2.24697999e-01 5.77758009e-01 2.30523648e-01\n",
      " 3.41170232e-01 1.45805031e-01 1.50003113e-01 5.58930060e-01\n",
      " 3.77393022e-01 7.40903062e-01 9.02356848e-01 2.37250775e-02\n",
      " 5.41041451e-01 6.72362278e-01 9.52226285e-01 3.89357603e-01]\n",
      "The technology vector for the 1st company:  [0.57796362 0.88809321 0.78450869 0.30884423 0.24070557 0.69601115\n",
      " 0.62445858 0.86952996 0.27349223 0.25588849 0.00219323 0.38313698\n",
      " 0.51849811 0.77837242 0.31866593 0.50288311 0.5390339  0.36230507\n",
      " 0.92398726 0.73570212 0.81570466 0.0524279  0.76432075 0.67046032\n",
      " 0.50737283 0.83374507 0.36615215 0.75917183 0.73247802 0.13180893\n",
      " 0.26940371 0.36202184 0.98442529 0.83127254 0.61853018 0.50447815\n",
      " 0.92688688 0.66806406 0.51437495 0.88084179 0.29606867 0.14332094\n",
      " 0.48236135 0.23691286 0.9375753  0.99274347 0.30391403 0.54910745\n",
      " 0.91907009 0.78607144 0.55324136 0.36443421 0.27490417 0.90935104\n",
      " 0.02740485 0.67464566 0.01541414 0.40012814 0.7178927  0.77740187\n",
      " 0.73303631 0.88146861 0.05591865 0.20595442 0.4703016  0.77153294\n",
      " 0.29789899 0.72635245 0.31185485 0.04985075 0.79062954 0.21812533\n",
      " 0.75613141 0.85409422 0.39994062 0.45461888 0.04020066 0.86936422\n",
      " 0.44357237 0.5588917  0.82312928 0.59060246 0.20218168 0.31510001\n",
      " 0.84230019 0.10645842 0.69589432 0.98518003 0.1202438  0.66044316\n",
      " 0.65175153 0.75904355 0.55148198 0.1354109  0.95597986 0.23011596\n",
      " 0.44496014 0.12658367 0.80043112 0.14734489 0.63793155 0.19546749\n",
      " 0.65716054 0.06593665 0.38998644 0.5188337  0.370002   0.32695801\n",
      " 0.00708886 0.5211433  0.61826788 0.35734163 0.8018226  0.33305943\n",
      " 0.77928789 0.26105043 0.26279253 0.50877984 0.68947969 0.21506101\n",
      " 0.52357131 0.43743908 0.14571932 0.14505621 0.22525089 0.48040393\n",
      " 0.0510165  0.33604327 0.41766123 0.99667215 0.00241673 0.87412917\n",
      " 0.54254341 0.08121382 0.46733556 0.5025766  0.83201431 0.52218607\n",
      " 0.37227338 0.07028847 0.45196386 0.91049394 0.06246044 0.86638346\n",
      " 0.63827904 0.93225733 0.4741083  0.17161564 0.61707828 0.53450591\n",
      " 0.39656714 0.84714551 0.40496042 0.78106879 0.65013446 0.05913684\n",
      " 0.01563254 0.57817781 0.52511242 0.11142856 0.69413397 0.43568338\n",
      " 0.47419458 0.66943011 0.7733484  0.17351425 0.95136308 0.83360124\n",
      " 0.46984018 0.13674525 0.71133557 0.89208461 0.75699937 0.73710552\n",
      " 0.15528059 0.20029774 0.36180952 0.80134639 0.94705221 0.01498944\n",
      " 0.53253798 0.35683417 0.00573399 0.63850996 0.279829   0.59917973\n",
      " 0.97573935 0.0353378  0.75773924 0.68753232 0.45876033 0.08850869\n",
      " 0.74731553 0.96564985 0.43776834 0.4360611  0.0561624  0.4008126\n",
      " 0.39483654 0.4684582  0.11711837 0.0164634  0.89306501 0.9255508\n",
      " 0.43596264 0.39915251 0.11848156 0.18696533 0.59496213 0.31753179\n",
      " 0.25948728 0.77086413 0.97927187 0.41092807 0.28984154 0.29739532\n",
      " 0.62607488 0.34476102 0.95330029 0.51354179 0.87407801 0.94436108\n",
      " 0.20794222 0.8286885  0.35593126 0.2806436  0.66884231 0.91809778\n",
      " 0.58435948 0.67295896 0.98974061 0.80599223 0.69267777 0.81202283\n",
      " 0.13903795 0.0456182  0.25035842 0.0431968  0.8004649  0.92558052\n",
      " 0.72560513 0.98703669 0.12608333 0.73105842 0.59210083 0.78839224\n",
      " 0.41653046 0.26654388 0.66063713 0.33699369 0.96264928 0.0882031\n",
      " 0.58426629 0.88952151 0.30394125 0.19410578 0.41204027 0.9115996\n",
      " 0.95265445 0.68233873 0.57863586 0.16358373 0.1158303  0.98846831\n",
      " 0.60733873 0.28358484 0.62870109 0.17527536 0.70848403 0.07330017\n",
      " 0.66877732 0.92483348 0.55101319 0.68106113 0.99215758 0.60328886\n",
      " 0.793367   0.66744716 0.70956205 0.58728026 0.60889674 0.69248783\n",
      " 0.95913271 0.48816125 0.45957819 0.90974987 0.85381825 0.62969714\n",
      " 0.04472347 0.59029007 0.05695182 0.91233054 0.45873713 0.12014388\n",
      " 0.60846954 0.6984679  0.64157311 0.1944686  0.8107046  0.92836537]\n"
     ]
    }
   ],
   "source": [
    "print('The market vector for the 1st company: ', M_dict[Company_ids[0]])\n",
    "print('The technology vector for the 1st company: ', T_dict[Company_ids[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to use pytorch, which is a commonly used deep learning package, to generate the concordance matrix. Patent2Product is used to train the concordance matrix $W$ which converts **Technology vectors** to **Market vectors**, such that $m = Wt$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Patent2Product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########0##########\n",
      "loss\":  0.40203569412231444\n",
      "##########1##########\n",
      "loss\":  0.32211015120148656\n",
      "##########2##########\n",
      "loss\":  0.2582720617949963\n",
      "##########3##########\n",
      "loss\":  0.210417812243104\n",
      "##########4##########\n",
      "loss\":  0.17619522608816623\n",
      "##########5##########\n",
      "loss\":  0.15317783549427985\n",
      "##########6##########\n",
      "loss\":  0.13926016755402087\n",
      "##########7##########\n",
      "loss\":  0.1326439692080021\n",
      "##########8##########\n",
      "loss\":  0.13163605596870184\n",
      "##########9##########\n",
      "loss\":  0.13470597714185714\n",
      "##########10##########\n",
      "loss\":  0.14039706587791442\n",
      "##########11##########\n",
      "loss\":  0.14728813104331492\n",
      "##########12##########\n",
      "loss\":  0.1542786018550396\n",
      "##########13##########\n",
      "loss\":  0.16074289724230767\n",
      "##########14##########\n",
      "loss\":  0.1664618057012558\n",
      "##########15##########\n",
      "loss\":  0.17146515615284444\n",
      "##########16##########\n",
      "loss\":  0.17579629838466646\n",
      "##########17##########\n",
      "loss\":  0.17933218620717525\n",
      "##########18##########\n",
      "loss\":  0.18177206993103026\n",
      "##########19##########\n",
      "loss\":  0.18272810913622378\n",
      "##########20##########\n",
      "loss\":  0.18192838862538338\n",
      "##########21##########\n",
      "loss\":  0.1794534882158041\n",
      "##########22##########\n",
      "loss\":  0.17575169675052166\n",
      "##########23##########\n",
      "loss\":  0.17138722635805606\n",
      "##########24##########\n",
      "loss\":  0.16678178012371064\n",
      "##########25##########\n",
      "loss\":  0.1621701154112816\n",
      "##########26##########\n",
      "loss\":  0.15772846780717373\n",
      "##########27##########\n",
      "loss\":  0.15368398316204548\n",
      "##########28##########\n",
      "loss\":  0.1502924518659711\n",
      "##########29##########\n",
      "loss\":  0.14773421112447976\n",
      "##########30##########\n",
      "loss\":  0.14602709449827672\n",
      "##########31##########\n",
      "loss\":  0.1449939864128828\n",
      "##########32##########\n",
      "loss\":  0.1442972170561552\n",
      "##########33##########\n",
      "loss\":  0.143551045358181\n",
      "##########34##########\n",
      "loss\":  0.14245316408574582\n",
      "##########35##########\n",
      "loss\":  0.14084431767463684\n",
      "##########36##########\n",
      "loss\":  0.13870013184845448\n",
      "##########37##########\n",
      "loss\":  0.13611078634858131\n",
      "##########38##########\n",
      "loss\":  0.13325934410095214\n",
      "##########39##########\n",
      "loss\":  0.1303881173953414\n",
      "##########40##########\n",
      "loss\":  0.12775436650961638\n",
      "##########41##########\n",
      "loss\":  0.12557332135736943\n",
      "##########42##########\n",
      "loss\":  0.123951001688838\n",
      "##########43##########\n",
      "loss\":  0.12284805815666915\n",
      "##########44##########\n",
      "loss\":  0.1221096858009696\n",
      "##########45##########\n",
      "loss\":  0.12153223905712367\n",
      "##########46##########\n",
      "loss\":  0.12092208825051784\n",
      "##########47##########\n",
      "loss\":  0.12013009227812291\n",
      "##########48##########\n",
      "loss\":  0.11906670648604631\n",
      "##########49##########\n",
      "loss\":  0.11770252581685782\n",
      "##########50##########\n",
      "loss\":  0.11605541910976172\n",
      "##########51##########\n",
      "loss\":  0.11417662210762501\n",
      "##########52##########\n",
      "loss\":  0.11214854057878255\n",
      "##########53##########\n",
      "loss\":  0.11008855108171701\n",
      "##########54##########\n",
      "loss\":  0.10813716311007739\n",
      "##########55##########\n",
      "loss\":  0.10641914863139391\n",
      "##########56##########\n",
      "loss\":  0.10499824155122042\n",
      "##########57##########\n",
      "loss\":  0.10385434631258249\n",
      "##########58##########\n",
      "loss\":  0.10288994301110506\n",
      "##########59##########\n",
      "loss\":  0.10195285372436047\n",
      "##########60##########\n",
      "loss\":  0.10086963210254908\n",
      "##########61##########\n",
      "loss\":  0.09949084732681512\n",
      "##########62##########\n",
      "loss\":  0.09773622117936612\n",
      "##########63##########\n",
      "loss\":  0.09561721798032523\n",
      "##########64##########\n",
      "loss\":  0.09322606150060891\n",
      "##########65##########\n",
      "loss\":  0.09070101864635945\n",
      "##########66##########\n",
      "loss\":  0.0881881319731474\n",
      "##########67##########\n",
      "loss\":  0.08581736098974943\n",
      "##########68##########\n",
      "loss\":  0.08369579039514065\n",
      "##########69##########\n",
      "loss\":  0.08190537594258786\n",
      "##########70##########\n",
      "loss\":  0.08049430727958679\n",
      "##########71##########\n",
      "loss\":  0.07946549702435732\n",
      "##########72##########\n",
      "loss\":  0.07877311201766134\n",
      "##########73##########\n",
      "loss\":  0.07833155259490013\n",
      "##########74##########\n",
      "loss\":  0.07803217235952616\n",
      "##########75##########\n",
      "loss\":  0.07776048375293612\n",
      "##########76##########\n",
      "loss\":  0.07740878943353892\n",
      "##########77##########\n",
      "loss\":  0.07688605971634388\n",
      "##########78##########\n",
      "loss\":  0.07612770160660148\n",
      "##########79##########\n",
      "loss\":  0.07510289642959833\n",
      "##########80##########\n",
      "loss\":  0.07381755154579878\n",
      "##########81##########\n",
      "loss\":  0.07231235137209296\n",
      "##########82##########\n",
      "loss\":  0.07065392522141338\n",
      "##########83##########\n",
      "loss\":  0.0689215406216681\n",
      "##########84##########\n",
      "loss\":  0.06719495898112654\n",
      "##########85##########\n",
      "loss\":  0.0655464611761272\n",
      "##########86##########\n",
      "loss\":  0.06403580192476512\n",
      "##########87##########\n",
      "loss\":  0.06270424403250217\n",
      "##########88##########\n",
      "loss\":  0.06156786499544978\n",
      "##########89##########\n",
      "loss\":  0.06061505429446697\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Patent2Product(nn.Module):\n",
    "    def __init__(self, hidden_size=300):\n",
    "        super(Patent2Product, self).__init__()\n",
    "        self.W = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, patent_emb):\n",
    "        return self.W(patent_emb)\n",
    "\n",
    "\n",
    "model = Patent2Product()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-6, eps=1e-8)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(90):\n",
    "    print('##########' + str(epoch) + '##########')\n",
    "    loss_values = 0\n",
    "    n = len(Company_ids)\n",
    "    for idx in Company_ids:\n",
    "    \n",
    "        patent_rep = T_dict[idx] #rep -> representation\n",
    "        product_rep = M_dict[idx]\n",
    "    \n",
    "        patent_rep = torch.tensor(patent_rep)\n",
    "        product_rep = torch.tensor(product_rep)\n",
    "    \n",
    "        pred_product_rep = model(patent_rep.float())\n",
    "    \n",
    "        output = loss(pred_product_rep, product_rep.float())\n",
    "    \n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        loss_values += output.item()\n",
    "    print('loss\": ', loss_values/n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########0##########\n",
      "loss\":  0.4069836397469044\n",
      "##########1##########\n",
      "loss\":  0.3267474329471588\n",
      "##########2##########\n",
      "loss\":  0.2627543643862009\n",
      "##########3##########\n",
      "loss\":  0.2149646704643965\n",
      "##########4##########\n",
      "loss\":  0.18085583582520484\n",
      "##########5##########\n",
      "loss\":  0.15809414960443974\n",
      "##########6##########\n",
      "loss\":  0.14453076124191283\n",
      "##########7##########\n",
      "loss\":  0.13786610692739487\n",
      "##########8##########\n",
      "loss\":  0.1361248792335391\n",
      "##########9##########\n",
      "loss\":  0.13788298938423396\n",
      "##########10##########\n",
      "loss\":  0.14194734521210195\n",
      "##########11##########\n",
      "loss\":  0.14733989078551532\n",
      "##########12##########\n",
      "loss\":  0.15335064630955458\n",
      "##########13##########\n",
      "loss\":  0.15937602568417789\n",
      "##########14##########\n",
      "loss\":  0.16488921631127595\n",
      "##########15##########\n",
      "loss\":  0.16955834299325942\n",
      "##########16##########\n",
      "loss\":  0.17323443289846183\n",
      "##########17##########\n",
      "loss\":  0.17586947567760944\n",
      "##########18##########\n",
      "loss\":  0.17746883533895016\n",
      "##########19##########\n",
      "loss\":  0.1781368339061737\n",
      "##########20##########\n",
      "loss\":  0.17813165351748467\n",
      "##########21##########\n",
      "loss\":  0.177751489803195\n",
      "##########22##########\n",
      "loss\":  0.1771141105145216\n",
      "##########23##########\n",
      "loss\":  0.17606406286358833\n",
      "##########24##########\n",
      "loss\":  0.17430505380034447\n",
      "##########25##########\n",
      "loss\":  0.17163436472415924\n",
      "##########26##########\n",
      "loss\":  0.16808540888130666\n",
      "##########27##########\n",
      "loss\":  0.16391587778925895\n",
      "##########28##########\n",
      "loss\":  0.1594987415522337\n",
      "##########29##########\n",
      "loss\":  0.155181988440454\n",
      "##########30##########\n",
      "loss\":  0.15119683362543582\n",
      "##########31##########\n",
      "loss\":  0.14765129785984754\n",
      "##########32##########\n",
      "loss\":  0.1445465175434947\n",
      "##########33##########\n",
      "loss\":  0.14178607158362866\n",
      "##########34##########\n",
      "loss\":  0.13922872330993413\n",
      "##########35##########\n",
      "loss\":  0.13678545635193587\n",
      "##########36##########\n",
      "loss\":  0.13448643207550048\n",
      "##########37##########\n",
      "loss\":  0.13246519166976214\n",
      "##########38##########\n",
      "loss\":  0.13087949104607105\n",
      "##########39##########\n",
      "loss\":  0.12982180409133434\n",
      "##########40##########\n",
      "loss\":  0.1292604536935687\n",
      "##########41##########\n",
      "loss\":  0.12903143025934696\n",
      "##########42##########\n",
      "loss\":  0.12888017643243074\n",
      "##########43##########\n",
      "loss\":  0.12853264957666397\n",
      "##########44##########\n",
      "loss\":  0.12776969958096743\n",
      "##########45##########\n",
      "loss\":  0.12648365929722785\n",
      "##########46##########\n",
      "loss\":  0.12469631213694811\n",
      "##########47##########\n",
      "loss\":  0.12253396630287171\n",
      "##########48##########\n",
      "loss\":  0.12017883338034153\n",
      "##########49##########\n",
      "loss\":  0.11781930081546306\n",
      "##########50##########\n",
      "loss\":  0.11560821704566479\n",
      "##########51##########\n",
      "loss\":  0.11363022111356258\n",
      "##########52##########\n",
      "loss\":  0.11189207594841719\n",
      "##########53##########\n",
      "loss\":  0.11034556858241558\n",
      "##########54##########\n",
      "loss\":  0.1089274463057518\n",
      "##########55##########\n",
      "loss\":  0.10759073287248612\n",
      "##########56##########\n",
      "loss\":  0.10631278716027737\n",
      "##########57##########\n",
      "loss\":  0.10508286446332932\n",
      "##########58##########\n",
      "loss\":  0.10388507816940545\n",
      "##########59##########\n",
      "loss\":  0.102691247202456\n",
      "##########60##########\n",
      "loss\":  0.10146332539618015\n",
      "##########61##########\n",
      "loss\":  0.10015547472983599\n",
      "##########62##########\n",
      "loss\":  0.09871601648628711\n",
      "##########63##########\n",
      "loss\":  0.0970993460714817\n",
      "##########64##########\n",
      "loss\":  0.09528626091778278\n",
      "##########65##########\n",
      "loss\":  0.09330031208693981\n",
      "##########66##########\n",
      "loss\":  0.09121058247983456\n",
      "##########67##########\n",
      "loss\":  0.0891170822456479\n",
      "##########68##########\n",
      "loss\":  0.08712391555309296\n",
      "##########69##########\n",
      "loss\":  0.08531132906675339\n",
      "##########70##########\n",
      "loss\":  0.08371903993189335\n",
      "##########71##########\n",
      "loss\":  0.0823485479131341\n",
      "##########72##########\n",
      "loss\":  0.08117889206856489\n",
      "##########73##########\n",
      "loss\":  0.08018074296414852\n",
      "##########74##########\n",
      "loss\":  0.07931923717260361\n",
      "##########75##########\n",
      "loss\":  0.07854978851974011\n",
      "##########76##########\n",
      "loss\":  0.077818065546453\n",
      "##########77##########\n",
      "loss\":  0.07706837683916092\n",
      "##########78##########\n",
      "loss\":  0.07625437445938588\n",
      "##########79##########\n",
      "loss\":  0.07534495262429118\n",
      "##########80##########\n",
      "loss\":  0.07432459415867924\n",
      "##########81##########\n",
      "loss\":  0.07319137858226896\n",
      "##########82##########\n",
      "loss\":  0.07195538161322475\n",
      "##########83##########\n",
      "loss\":  0.07063705673441291\n",
      "##########84##########\n",
      "loss\":  0.0692647735029459\n",
      "##########85##########\n",
      "loss\":  0.06787334723398089\n",
      "##########86##########\n",
      "loss\":  0.06650311684235931\n",
      "##########87##########\n",
      "loss\":  0.06519587058573961\n",
      "##########88##########\n",
      "loss\":  0.0639867202565074\n",
      "##########89##########\n",
      "loss\":  0.06289620250463486\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Product2Patent(nn.Module):\n",
    "    def __init__(self, hidden_size=300):\n",
    "        super(Product2Patent, self).__init__()\n",
    "        self.W = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, product_emb):\n",
    "        return self.W(product_emb)\n",
    "\n",
    "\n",
    "model = Product2Patent()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-6, eps=1e-8)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(90):\n",
    "    print('##########' + str(epoch) + '##########')\n",
    "    loss_values = 0\n",
    "    n = len(Company_ids)\n",
    "    for idx in Company_ids:\n",
    "    \n",
    "        patent_rep = T_dict[idx] #rep -> representation\n",
    "        product_rep = M_dict[idx]\n",
    "    \n",
    "        patent_rep = torch.tensor(patent_rep)\n",
    "        product_rep = torch.tensor(product_rep)\n",
    "    \n",
    "        pred_patent_rep = model(product_rep.float())\n",
    "    \n",
    "        output = loss(pred_patent_rep, patent_rep.float())\n",
    "    \n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        loss_values += output.item()\n",
    "    print('loss\": ', loss_values/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the trained matrix, and convert it to a numpy format: \n",
    "\n",
    "**Note: model -> Product2Patent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00984566, -0.01518547,  0.02941971, ...,  0.01251339,\n",
       "         0.01823718, -0.00455915],\n",
       "       [ 0.04339917, -0.03286355,  0.02020139, ..., -0.02475692,\n",
       "        -0.01887817, -0.03694284],\n",
       "       [ 0.0175809 , -0.01054933, -0.00878088, ..., -0.00897275,\n",
       "        -0.01198722,  0.03364225],\n",
       "       ...,\n",
       "       [ 0.02596598,  0.06618389,  0.01389562, ...,  0.01874409,\n",
       "         0.03300765,  0.01045165],\n",
       "       [ 0.02547042,  0.05104292,  0.04433365, ..., -0.05745807,\n",
       "        -0.018433  ,  0.02381825],\n",
       "       [-0.01368219,  0.01941934,  0.02370436, ...,  0.06494612,\n",
       "         0.00156734,  0.01528808]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = model.W.weight.detach().numpy()\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with pytorch, you can direct use it by, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated technology vector:  tensor([ 0.6946,  0.1388,  0.5107,  0.3835,  0.4541,  0.4891,  0.5140,  0.6047,\n",
      "         0.3468,  0.3065,  0.6402,  0.3535,  0.5290,  0.6260,  0.2615,  0.3217,\n",
      "         0.3615,  0.4065,  0.6230,  0.7885,  0.6999,  0.3399,  0.3948,  0.5525,\n",
      "         0.7400,  0.6989,  0.5550,  0.5431,  0.4952,  0.1844,  0.2025,  0.5989,\n",
      "         0.7192,  0.5874,  0.3918,  0.3840,  0.5281,  0.8388,  0.5468,  0.6795,\n",
      "         0.3342,  0.4795,  0.4885,  0.3230,  0.8537,  0.6447,  0.4012,  0.4875,\n",
      "         0.6578,  0.6398,  0.2538,  0.4650,  0.4954,  0.6666,  0.5745,  0.6280,\n",
      "         0.6229,  0.5106,  0.6595,  0.5387,  0.6717,  0.2463,  0.3253,  0.3853,\n",
      "         0.3710,  0.3988,  0.5328,  0.5303,  0.3543,  0.4153,  0.6800,  0.4707,\n",
      "         0.5667,  0.7581,  0.5241,  0.5138,  0.2660,  0.6601,  0.1587,  0.6814,\n",
      "         0.7396,  0.3213,  0.3678,  0.7020,  0.5474,  0.3194,  0.6759,  0.7029,\n",
      "         0.4347,  0.3614,  0.4995,  0.6427,  0.3803,  0.5238,  0.5263,  0.3667,\n",
      "         0.5165,  0.1081,  0.5806,  0.2870,  0.2398,  0.2627,  0.4151,  0.3390,\n",
      "         0.5293,  0.6764,  0.4098,  0.4088,  0.4407,  0.7331,  0.6974,  0.4171,\n",
      "         0.2677,  0.2113,  0.5416,  0.3057,  0.4197,  0.5784,  0.4562,  0.4915,\n",
      "         0.2940,  0.4650,  0.3340,  0.2900,  0.5062,  0.6371,  0.5754,  0.5004,\n",
      "         0.4163,  0.5672,  0.3358,  0.8574,  0.4626,  0.2775,  0.5807,  0.2307,\n",
      "         0.6895,  0.4853,  0.6307,  0.3283,  0.4184,  0.7236,  0.0367,  0.1584,\n",
      "         0.3932,  0.6584,  0.5507,  0.1976,  0.3099,  0.7070,  0.5745,  0.6041,\n",
      "         0.4151,  0.3544,  0.7434,  0.2792,  0.5075, -0.0462,  0.6322,  0.5234,\n",
      "         0.4258,  0.4695,  0.4014,  0.7872,  0.6640,  0.2746,  0.7255,  0.6081,\n",
      "         0.6470,  0.5056,  0.5081,  0.6194,  0.4254,  0.4237,  0.0860,  0.3543,\n",
      "         0.6811,  0.5693,  0.6050,  0.4194,  0.4253,  0.2147,  0.3781,  0.6040,\n",
      "         0.3643,  0.5169,  0.6785,  0.2926,  0.5887,  0.4537,  0.3536,  0.3817,\n",
      "         0.6272,  0.8133,  0.7351,  0.4311,  0.2867,  0.3076,  0.3713,  0.5079,\n",
      "         0.5172,  0.3032,  0.5032,  0.6625,  0.2966,  0.4261,  0.1218,  0.4035,\n",
      "         0.5999,  0.6274,  0.3677,  0.9362,  0.6577,  0.4851,  0.5558,  0.4129,\n",
      "         0.7992,  0.4594,  0.6546,  0.6104,  0.6662,  0.5598,  0.3055,  0.6429,\n",
      "         0.3971,  0.5718,  0.3483,  0.6877,  0.4510,  0.6024,  0.8334,  0.7291,\n",
      "         0.3397,  0.7364,  0.5661,  0.1313,  0.4541,  0.3828,  0.6516,  0.5243,\n",
      "         0.6577,  0.8964,  0.2275,  0.6071,  0.6712,  0.3324,  0.4885,  0.4690,\n",
      "         0.9209,  0.6712,  0.7044,  0.3928,  0.5537,  0.8008,  0.6595,  0.1906,\n",
      "         0.5299,  0.6955,  0.7118,  0.5905,  0.6220,  0.2924,  0.4065,  0.8648,\n",
      "         0.3057,  0.5967,  0.4890,  0.4054,  0.8216,  0.1609,  0.6937,  0.6777,\n",
      "         0.5889,  0.3012,  0.6070,  0.5483,  0.5568,  0.5475,  0.2900,  0.3746,\n",
      "         0.4411,  0.4668,  0.7197,  0.5554,  0.6611,  0.6853,  0.6831,  0.5379,\n",
      "         0.2501,  0.6439,  0.4733,  0.6201,  0.3378,  0.4744,  0.7359,  0.8711,\n",
      "         0.6437,  0.5342,  0.4305,  0.7310], grad_fn=<SqueezeBackward3>)\n",
      "The true technology vector:  [0.57796362 0.88809321 0.78450869 0.30884423 0.24070557 0.69601115\n",
      " 0.62445858 0.86952996 0.27349223 0.25588849 0.00219323 0.38313698\n",
      " 0.51849811 0.77837242 0.31866593 0.50288311 0.5390339  0.36230507\n",
      " 0.92398726 0.73570212 0.81570466 0.0524279  0.76432075 0.67046032\n",
      " 0.50737283 0.83374507 0.36615215 0.75917183 0.73247802 0.13180893\n",
      " 0.26940371 0.36202184 0.98442529 0.83127254 0.61853018 0.50447815\n",
      " 0.92688688 0.66806406 0.51437495 0.88084179 0.29606867 0.14332094\n",
      " 0.48236135 0.23691286 0.9375753  0.99274347 0.30391403 0.54910745\n",
      " 0.91907009 0.78607144 0.55324136 0.36443421 0.27490417 0.90935104\n",
      " 0.02740485 0.67464566 0.01541414 0.40012814 0.7178927  0.77740187\n",
      " 0.73303631 0.88146861 0.05591865 0.20595442 0.4703016  0.77153294\n",
      " 0.29789899 0.72635245 0.31185485 0.04985075 0.79062954 0.21812533\n",
      " 0.75613141 0.85409422 0.39994062 0.45461888 0.04020066 0.86936422\n",
      " 0.44357237 0.5588917  0.82312928 0.59060246 0.20218168 0.31510001\n",
      " 0.84230019 0.10645842 0.69589432 0.98518003 0.1202438  0.66044316\n",
      " 0.65175153 0.75904355 0.55148198 0.1354109  0.95597986 0.23011596\n",
      " 0.44496014 0.12658367 0.80043112 0.14734489 0.63793155 0.19546749\n",
      " 0.65716054 0.06593665 0.38998644 0.5188337  0.370002   0.32695801\n",
      " 0.00708886 0.5211433  0.61826788 0.35734163 0.8018226  0.33305943\n",
      " 0.77928789 0.26105043 0.26279253 0.50877984 0.68947969 0.21506101\n",
      " 0.52357131 0.43743908 0.14571932 0.14505621 0.22525089 0.48040393\n",
      " 0.0510165  0.33604327 0.41766123 0.99667215 0.00241673 0.87412917\n",
      " 0.54254341 0.08121382 0.46733556 0.5025766  0.83201431 0.52218607\n",
      " 0.37227338 0.07028847 0.45196386 0.91049394 0.06246044 0.86638346\n",
      " 0.63827904 0.93225733 0.4741083  0.17161564 0.61707828 0.53450591\n",
      " 0.39656714 0.84714551 0.40496042 0.78106879 0.65013446 0.05913684\n",
      " 0.01563254 0.57817781 0.52511242 0.11142856 0.69413397 0.43568338\n",
      " 0.47419458 0.66943011 0.7733484  0.17351425 0.95136308 0.83360124\n",
      " 0.46984018 0.13674525 0.71133557 0.89208461 0.75699937 0.73710552\n",
      " 0.15528059 0.20029774 0.36180952 0.80134639 0.94705221 0.01498944\n",
      " 0.53253798 0.35683417 0.00573399 0.63850996 0.279829   0.59917973\n",
      " 0.97573935 0.0353378  0.75773924 0.68753232 0.45876033 0.08850869\n",
      " 0.74731553 0.96564985 0.43776834 0.4360611  0.0561624  0.4008126\n",
      " 0.39483654 0.4684582  0.11711837 0.0164634  0.89306501 0.9255508\n",
      " 0.43596264 0.39915251 0.11848156 0.18696533 0.59496213 0.31753179\n",
      " 0.25948728 0.77086413 0.97927187 0.41092807 0.28984154 0.29739532\n",
      " 0.62607488 0.34476102 0.95330029 0.51354179 0.87407801 0.94436108\n",
      " 0.20794222 0.8286885  0.35593126 0.2806436  0.66884231 0.91809778\n",
      " 0.58435948 0.67295896 0.98974061 0.80599223 0.69267777 0.81202283\n",
      " 0.13903795 0.0456182  0.25035842 0.0431968  0.8004649  0.92558052\n",
      " 0.72560513 0.98703669 0.12608333 0.73105842 0.59210083 0.78839224\n",
      " 0.41653046 0.26654388 0.66063713 0.33699369 0.96264928 0.0882031\n",
      " 0.58426629 0.88952151 0.30394125 0.19410578 0.41204027 0.9115996\n",
      " 0.95265445 0.68233873 0.57863586 0.16358373 0.1158303  0.98846831\n",
      " 0.60733873 0.28358484 0.62870109 0.17527536 0.70848403 0.07330017\n",
      " 0.66877732 0.92483348 0.55101319 0.68106113 0.99215758 0.60328886\n",
      " 0.793367   0.66744716 0.70956205 0.58728026 0.60889674 0.69248783\n",
      " 0.95913271 0.48816125 0.45957819 0.90974987 0.85381825 0.62969714\n",
      " 0.04472347 0.59029007 0.05695182 0.91233054 0.45873713 0.12014388\n",
      " 0.60846954 0.6984679  0.64157311 0.1944686  0.8107046  0.92836537]\n"
     ]
    }
   ],
   "source": [
    "m = M_dict[0]\n",
    "t = T_dict[0]\n",
    "\n",
    "m = torch.tensor(m)\n",
    "t_est = model(m.float())\n",
    "\n",
    "print('The estimated technology vector: ', t_est)\n",
    "print('The true technology vector: ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caveats: we did not split train and test sets in this toy example, because these vectors are randomly generated. One needs to split train and test sets in real cases.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
